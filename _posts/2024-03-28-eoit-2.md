---
layout: post
title:  "Elements of Information Theory - Ch. 2"
date: 2024-03-28
last_modified_at: 2024-03-31
categories: [Information Theory]
---

&emsp;This is the start of a series of posts where I will be taking notes on key ideas and theorems as well as solving problems from *Elements of Information Theory* by Cover and Thomas. Chapter 2 introduces many fundamental concepts that lay the foundation for later chapters. Hence, this post is possibly quite lengthy compared to future posts in this series.

## Entropy
#### Definition
&emsp;*Entropy* is a measure of the average uncertainty in a random variable. Let $X$ be a discrete random variable with alphabet $\mathcal{X}$ with probability mass function $p(x) = \Pr\\{X = x\\}$, $x \in \mathcal{X}$. The entropy $H(X)$ of $X$ is defined as

$$ H(X) = \sum_{x \in \mathcal{X}}p(x)\log{\frac{1}{p(x)}} = -\sum_{x \in \mathcal{X}}p(x)\log{p(x)}. $$

&emsp;We will always use $\log$ to the base 2 and entropy is expressed in *bits*. In this case, the entropy $H(X)$ is the *average* number of bits required to describe $X$. When using $\ln$, entropy is expressed in *nats*. If the base of the logarithm is $b$, we denote the entropy as $H_b(X)$.

&emsp;We define the expection of random variable $g(X)$ as 

$$ Eg(x) = \sum_{x \in \mathcal{X}} g(x)p(x). $$

&emsp;So, the entropy of $X$ can also be defined as 

$$ H(X) = E\log{\frac{1}{p(X)}}, $$ 

where $g(X) = \log{\frac{1}{p(X)}}$.

#### Properties

&emsp;1. $H(X) \geq 0$

&emsp;&emsp;Proof: $0 \leq p(x) \leq 1 \Rightarrow \log{\frac{1}{p(x)}} \geq 0$.

&emsp;2. $H_b(X) = (\log_b{a})H_a(X)$

&emsp;&emsp;Proof: $\log_b{p} = (\log_b{a}) \log_a{p}$.

#### Example 1

Let us consider the case of a fair six-sided die. The entropy, in bits, of the outcome of the die roll $X$ can be computed as follows:

$$ H(X) = \sum_{i = 1}^6 p(x_i)\log_2{\frac{1}{p(x_i)}} = \sum_{i = 1}^6 \frac{1}{6} \log_2{6} = \log_2{6} \approx 2.58. $$

This means that, on average, $2.58$ bits are required to describe the outcome of the die roll. We can list out bit representations of the possible outcomes:

<table style="width: 35%; text-align: center; margin-right: auto; margin-left: auto;">
  <tr>
    <th>Outcome</th>
    <th>Bit Representation</th>
  </tr>
  <tr>
    <td>1</td>
    <td>000</td>
  </tr>
  <tr>
    <td>2</td>
    <td>001</td>
  </tr>
  <tr>
    <td>3</td>
    <td>010</td>
  </tr>
  <tr>
    <td>4</td>
    <td>011</td>
  </tr>
  <tr>
    <td>5</td>
    <td>100</td>
  </tr>
  <tr>
    <td>6</td>
    <td>101</td>
  </tr>
</table>

We note that $110$ and $111$ are not used.

#### Example 2
&emsp;Let $X$ be a Bernoulli random variable, where $\Pr\\{X = 1\\} = p$ and $\Pr\\{X = 0 \\} = 1 - p$. The *binary entropy function* $H(p)$ is the entropy of a Bernoulli trial, given by
$$ H(X) = H(p) = -p\log_2{p} - (1 - p)\log_2{(1-p)}. $$
When $p = 0$ or $p = 1$, $H(p) = 0$. Intuitively, if we can say with certainty that one outcome or the other will occur, the entropy, or average uncertainty, is $0$. When $p = 0.5$, the entropy will be at the maximum of $1$ bit. Intermediate values of $p$ will result in an entropy less than $1$ bit as we are more certain that one of the outcomes will occur. From this, we can conclude that $H(p)$ is concave in $p$. The notions of concave and convex functions are explored in detail in later sections.

## Joint Entropy and Conditional Entropy

## Relative Entropy and Mutual Information

## Relationship Between Entropy and Mutual Information

## Chain Rules

## Jensen's Inequality

## Log Sum Inequality

## Data-Processing Inequality

## Sufficient Statistics

## Fano's Inequality